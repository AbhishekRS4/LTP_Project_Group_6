{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import ast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(example):\n",
    "    example[\"Labels\"] = [i for i in ast.literal_eval(example[\"Labels\"])]\n",
    "    return example\n",
    "\n",
    "valueeval23 = load_dataset(\"webis/Touche23-ValueEval\")\n",
    "training_dataset = valueeval23[\"training\"].map(convert_labels)\n",
    "validation_dataset = valueeval23[\"validation\"].map(convert_labels)\n",
    "# training_dataset['Labels']\n",
    "valueeval23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from local files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('task_1','dataset')\n",
    "arguments_training_filepath = os.path.join(data_dir, 'arguments-training.tsv')\n",
    "arguments_validation_filepath = os.path.join(data_dir, 'arguments-validation.tsv')\n",
    "arguments_validation_filepath_zhihu = os.path.join(data_dir, 'arguments-validation-zhihu.tsv')\n",
    "arguments_test_filepath = os.path.join(data_dir, 'arguments-test.tsv')\n",
    "\n",
    "labels_training_filepath = os.path.join(data_dir, 'labels-training.tsv')\n",
    "labels_validation_filepath = os.path.join(data_dir, 'labels-validation.tsv')\n",
    "labels_validation_filepath_zhihu = os.path.join(data_dir, 'labels-validation-zhihu.tsv')\n",
    "labels_test_filepath = os.path.join(data_dir, 'labels-test.tsv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_training = pd.read_csv(arguments_training_filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "arguments_validation = pd.read_csv(arguments_validation_filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "arguments_validation_zhihu = pd.read_csv(arguments_validation_filepath_zhihu, encoding='utf-8', sep='\\t', header=0)\n",
    "arguments_test = pd.read_csv(arguments_test_filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "\n",
    "\n",
    "labels_training = pd.read_csv(labels_training_filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "labels_validation = pd.read_csv(labels_validation_filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "labels_validation_zhihu = pd.read_csv(labels_validation_filepath_zhihu, encoding='utf-8', sep='\\t', header=0)\n",
    "labels_test = pd.read_csv(labels_test_filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "\n",
    "\n",
    "print(arguments_training.iloc[0], '\\n', labels_training.iloc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary_labels_to_string(df):\n",
    "    label_names = df.columns[1:]\n",
    "    string_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        binary_values = row.values[1:]\n",
    "        string_labels.append([label_names[i] for i, value in enumerate(binary_values) if value == 1])\n",
    "\n",
    "    df['String Labels'] = string_labels\n",
    "    return df\n",
    "\n",
    "def add_prompt_to_df(df):\n",
    "    prompt_format = \"Premise: {}\\n Stance: {}\\n Conclusion {}\\n. Which value category does it support?\\n\"\n",
    "    preprocessed_arguments = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        premise = row['Premise']\n",
    "        stance = row['Stance']\n",
    "        conclusion = row['Conclusion']\n",
    "        prompt = prompt_format.format(premise, stance, conclusion)\n",
    "        preprocessed_arguments.append(prompt)\n",
    "        \n",
    "    df['Prompt'] = preprocessed_arguments\n",
    "    return df\n",
    "\n",
    "def combine_columns(df_arguments, df_labels):\n",
    "    \"\"\"Combines the two `DataFrames` on column `Argument ID`\"\"\"\n",
    "    # Combine the two dataframes use for the labels only the column 'String Labels'\n",
    "    df_labels = df_labels[['Argument ID', 'String Labels']]\n",
    "    df_labels.columns = ['Argument ID', 'Labels']\n",
    "\n",
    "    return pd.merge(df_arguments, df_labels, on='Argument ID')\n",
    "\n",
    "def labels_to_multi_choice(labels):\n",
    "    \"\"\"Converts the labels to a multi choice format\"\"\"\n",
    "    multi_choice_format = \"{}: {}\"\n",
    "    multi_choice_options = []\n",
    "\n",
    "    for index, label in enumerate(labels):\n",
    "        multi_choice_option = multi_choice_format.format(chr(65 + index), label)\n",
    "        multi_choice_options.append(multi_choice_option)\n",
    "\n",
    "    return multi_choice_options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Prompt to the datafram and Convert the label to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique values for labels\n",
    "# print(labels_training.columns.unique())\n",
    "\n",
    "# use the unique calums as multi choice options for the prompt eg A: 1, B: 2, C: 3, D: 4\n",
    "\n",
    "labels = labels_training.columns.unique()\n",
    "print(labels)\n",
    "# remove Argument ID and String Labels from the list\n",
    "labels = [label for label in labels if label != 'Argument ID' and label != 'String Labels']\n",
    "\n",
    "multi_choice_options = labels_to_multi_choice(labels)\n",
    "print(labels,multi_choice_options)\n",
    "\n",
    "# can you generate a promt for argument consisting of premise, stance and conclusion where the model needs to predict a category value consisting out of 'Self-direction: thought', 'Self-direction: action',\n",
    "#        'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance',\n",
    "#        'Power: resources', 'Face', 'Security: personal', 'Security: societal',\n",
    "#        'Tradition', 'Conformity: rules', 'Conformity: interpersonal',\n",
    "#        'Humility', 'Benevolence: caring', 'Benevolence: dependability',\n",
    "#        'Universalism: concern', 'Universalism: nature',\n",
    "#        'Universalism: tolerance', 'Universalism: objectivity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_training = add_prompt_to_df(arguments_training)\n",
    "converted_labels = convert_binary_labels_to_string(labels_training)\n",
    "training_arg_labels = combine_columns(arguments_training, converted_labels)\n",
    "\n",
    "arguments_training['Prompt'].iloc[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the arguments and string labels into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument = arguments_training.iloc[1]\n",
    "print(argument)\n",
    "sample = f\"The premise: {argument['Premise']} is {argument['Stance']} conclusion: {argument['Conclusion']}. Which value category does it support? \\n (A) societal (B) rules (C) interpersonal (D) none of the above\"\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument = processed_arguments[1]\n",
    "argument = f\"{argument} (A) societal (B) rules (C) interpersonal (D) none of the above\"\n",
    "argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional, List\n",
    "# from transformers import pipeline\n",
    "\n",
    "# from transformers import pipeline\n",
    "\n",
    "# generator = pipeline(\"text2text-generation\", model=\"allenai/unifiedqa-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwdproject/LTP_Project_Group_6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "dataset_path = 'task_1/dataset/processed/dataset_dict/'\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
